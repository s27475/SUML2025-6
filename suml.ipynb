{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:44:15.111488Z",
     "start_time": "2026-01-18T12:44:15.095725Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.read_csv('openpowerlifting.csv')\n",
    "\n",
    "# Usuwamy tylko to, co naprawdę nieprzydatne do analizy\n",
    "to_drop = ['MeetID', 'Division', 'Place', 'Squat4Kg', 'Bench4Kg', 'Deadlift4Kg']\n",
    "df.drop(columns=[c for c in to_drop if c in df.columns], inplace=True)\n",
    "\n",
    "# Mapowanie płci na wartości numeryczne\n",
    "df['Sex'] = df['Sex'].map({'F': 0, 'M': 1})\n",
    "\n",
    "# Usuwanie duplikatów\n",
    "df.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:44:15.866925Z",
     "start_time": "2026-01-18T12:44:15.116413Z"
    }
   },
   "id": "4ad1161da869597c",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "               Name  Sex   Equipment   Age  BodyweightKg WeightClassKg  \\\n0  Angie Belk Terry    0       Wraps  47.0         59.60            60   \n1       Dawn Bogart    0  Single-ply  42.0         58.51            60   \n3       Dawn Bogart    0         Raw  42.0         58.51            60   \n4      Destiny Dula    0         Raw  18.0         63.68          67.5   \n5   Courtney Norris    0       Wraps  28.0         62.41          67.5   \n\n   BestSquatKg  BestBenchKg  BestDeadliftKg  TotalKg   Wilks  \n0        47.63        20.41           70.31   138.35  155.05  \n1       142.88        95.25          163.29   401.42  456.38  \n3          NaN        95.25             NaN    95.25  108.29  \n4          NaN        31.75           90.72   122.47  130.47  \n5       170.10        77.11          145.15   392.36  424.40  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Equipment</th>\n      <th>Age</th>\n      <th>BodyweightKg</th>\n      <th>WeightClassKg</th>\n      <th>BestSquatKg</th>\n      <th>BestBenchKg</th>\n      <th>BestDeadliftKg</th>\n      <th>TotalKg</th>\n      <th>Wilks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Angie Belk Terry</td>\n      <td>0</td>\n      <td>Wraps</td>\n      <td>47.0</td>\n      <td>59.60</td>\n      <td>60</td>\n      <td>47.63</td>\n      <td>20.41</td>\n      <td>70.31</td>\n      <td>138.35</td>\n      <td>155.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dawn Bogart</td>\n      <td>0</td>\n      <td>Single-ply</td>\n      <td>42.0</td>\n      <td>58.51</td>\n      <td>60</td>\n      <td>142.88</td>\n      <td>95.25</td>\n      <td>163.29</td>\n      <td>401.42</td>\n      <td>456.38</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dawn Bogart</td>\n      <td>0</td>\n      <td>Raw</td>\n      <td>42.0</td>\n      <td>58.51</td>\n      <td>60</td>\n      <td>NaN</td>\n      <td>95.25</td>\n      <td>NaN</td>\n      <td>95.25</td>\n      <td>108.29</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Destiny Dula</td>\n      <td>0</td>\n      <td>Raw</td>\n      <td>18.0</td>\n      <td>63.68</td>\n      <td>67.5</td>\n      <td>NaN</td>\n      <td>31.75</td>\n      <td>90.72</td>\n      <td>122.47</td>\n      <td>130.47</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Courtney Norris</td>\n      <td>0</td>\n      <td>Wraps</td>\n      <td>28.0</td>\n      <td>62.41</td>\n      <td>67.5</td>\n      <td>170.10</td>\n      <td>77.11</td>\n      <td>145.15</td>\n      <td>392.36</td>\n      <td>424.40</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:44:15.876573Z",
     "start_time": "2026-01-18T12:44:15.867861Z"
    }
   },
   "id": "bb31c0adc57f8851",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['age_imputer_model.pkl']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_age = ['Sex', 'BodyweightKg', 'BestSquatKg', 'BestBenchKg', 'BestDeadliftKg', 'TotalKg']\n",
    "\n",
    "df_known_age = df[df['Age'].notnull()].dropna(subset=features_age)\n",
    "df_missing_age = df[df['Age'].isnull()].dropna(subset=features_age)\n",
    "\n",
    "X_train_age = df_known_age[features_age]\n",
    "y_train_age = df_known_age['Age']\n",
    "\n",
    "reg_age = LinearRegression()\n",
    "reg_age.fit(X_train_age, y_train_age)\n",
    "\n",
    "predicted_ages = reg_age.predict(df_missing_age[features_age])\n",
    "df['ImputedAge'] = df['Age']\n",
    "df.loc[df_missing_age.index, 'ImputedAge'] = predicted_ages\n",
    "\n",
    "joblib.dump(reg_age, 'age_imputer_model.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:44:16.035670Z",
     "start_time": "2026-01-18T12:44:15.877966Z"
    }
   },
   "id": "fc976a7784dfbe0e",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bins = [0, 12, 16, 18, 22, 25, 30, 35, 40, 45, 50, 60, 70, 85, 100]\n",
    "labels = ['0-11y', '12-15y', '16-17y', '18-21y', '22-24y', '25-29y', '30-34y', '35-39y', '40-44y', '45-49y', '50-59y', '60-69y', '70-84y', '85-100y']\n",
    "\n",
    "df['AgeGroup'] = pd.cut(df['ImputedAge'], bins=bins, labels=labels, right=False)\n",
    "df['AgeGroup'] = df['AgeGroup'].cat.add_categories('Unknown').fillna('Unknown')\n",
    "\n",
    "# Usuwamy wiersze, gdzie wiek nadal jest nieznany\n",
    "df = df[df['AgeGroup'] != 'Unknown']\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:44:16.102684Z",
     "start_time": "2026-01-18T12:44:16.038270Z"
    }
   },
   "id": "9be10afa1b96553e",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20260118_124416\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:55 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T8103\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1\n",
      "CUDA Version:       CUDA is not available\n",
      "GPU Count:          WARNING: Exception was raised when calculating GPU count (AssertionError)\n",
      "Memory Avail:       1.57 GB / 8.00 GB (19.6%)\n",
      "Disk Space Avail:   19.53 GB / 228.27 GB (8.6%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme'  : New in v1.5: The state-of-the-art for tabular data. Massively better than 'best' on datasets <100000 samples by using new Tabular Foundation Models (TFMs) meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, TabDPT, and TabM. Requires a GPU and `pip install autogluon.tabular[tabarena]` to install TabPFN, TabICL, and TabDPT.\n",
      "\tpresets='best'     : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='best_v150': New in v1.5: Better quality than 'best' and 5x+ faster to train. Give it a try!\n",
      "\tpresets='high'     : Strong accuracy with fast inference speed.\n",
      "\tpresets='high_v150': New in v1.5: Better quality than 'high' and 5x+ faster to train. Give it a try!\n",
      "\tpresets='good'     : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'   : Fast training time, ideal for initial prototyping.\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (242446 samples, 15.52 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"/Users/dominikpiwowarczyk/Desktop/SUMLProjekt/SUML2025-6/AutogluonModels/ag-20260118_124416\"\n",
      "Train Data Rows:    242446\n",
      "Train Data Columns: 6\n",
      "Label Column:       Wilks\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (771.07, 13.73, 325.61457, 99.2417)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1617.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.10 MB (0.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 5 | ['BodyweightKg', 'BestSquatKg', 'BestBenchKg', 'BestDeadliftKg', 'TotalKg']\n",
      "\t\t('int', [])   : 1 | ['Sex']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 5 | ['BodyweightKg', 'BestSquatKg', 'BestBenchKg', 'BestDeadliftKg', 'TotalKg']\n",
      "\t\t('int', ['bool']) : 1 | ['Sex']\n",
      "\t0.2s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 9.48 MB (0.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.0103, Train Rows: 239948, Val Rows: 2498\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ...\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/1.6 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 2.87634\n",
      "[2000]\tvalid_set's rmse: 2.48418\n",
      "[3000]\tvalid_set's rmse: 2.31666\n",
      "[4000]\tvalid_set's rmse: 2.20185\n",
      "[5000]\tvalid_set's rmse: 2.13222\n",
      "[6000]\tvalid_set's rmse: 2.0692\n",
      "[7000]\tvalid_set's rmse: 2.02892\n",
      "[8000]\tvalid_set's rmse: 1.99394\n",
      "[9000]\tvalid_set's rmse: 1.96614\n",
      "[10000]\tvalid_set's rmse: 1.93991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.9399\t = Validation score   (-root_mean_squared_error)\n",
      "\t65.58s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/2.2 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 2.08864\n",
      "[2000]\tvalid_set's rmse: 1.86368\n",
      "[3000]\tvalid_set's rmse: 1.77042\n",
      "[4000]\tvalid_set's rmse: 1.71034\n",
      "[5000]\tvalid_set's rmse: 1.67836\n",
      "[6000]\tvalid_set's rmse: 1.66208\n",
      "[7000]\tvalid_set's rmse: 1.65458\n",
      "[8000]\tvalid_set's rmse: 1.63891\n",
      "[9000]\tvalid_set's rmse: 1.62913\n",
      "[10000]\tvalid_set's rmse: 1.63962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.6283\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.89s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\tWarning: Potentially not enough memory to safely train model. Estimated to require 1.117 GB out of 2.267 GB available memory (49.278%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo avoid this warning, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.28 to avoid the warning)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tFitting with cpus=8, gpus=0, mem=1.1/2.3 GB\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 168 due to low memory. Expected memory usage reduced from 26.63% -> 15.0% of available memory...\n",
      "\t-1.3365\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.87s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t-1.577\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 1.117 GB out of 2.142 GB available memory (52.167%)... (50.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.09 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train ExtraTreesMSE... Skipping this model.\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/2.1 GB\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.5.0`. \n",
      "Fitting model: XGBoost ...\n",
      "\tFitting with cpus=8, gpus=0\n",
      "\t-2.1852\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/2.1 GB\n",
      "/Users/dominikpiwowarczyk/Desktop/SUMLProjekt/SUML2025-6/venv/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t-8.5154\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tFitting with cpus=8, gpus=0, mem=0.1/2.1 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's rmse: 1.76956\n",
      "[2000]\tvalid_set's rmse: 1.62816\n",
      "[3000]\tvalid_set's rmse: 1.59344\n",
      "[4000]\tvalid_set's rmse: 1.56341\n",
      "[5000]\tvalid_set's rmse: 1.54229\n",
      "[6000]\tvalid_set's rmse: 1.52434\n",
      "[7000]\tvalid_set's rmse: 1.51321\n",
      "[8000]\tvalid_set's rmse: 1.50805\n",
      "[9000]\tvalid_set's rmse: 1.50528\n",
      "[10000]\tvalid_set's rmse: 1.50134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-1.5013\t = Validation score   (-root_mean_squared_error)\n",
      "\t176.01s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/2.0 GB\n",
      "\tEnsemble Weights: {'RandomForestMSE': 0.5, 'CatBoost': 0.273, 'LightGBMLarge': 0.227}\n",
      "\t-0.9841\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 395.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2417.1 rows/s (2498 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/dominikpiwowarczyk/Desktop/SUMLProjekt/SUML2025-6/AutogluonModels/ag-20260118_124416\")\n"
     ]
    }
   ],
   "source": [
    "target = 'Wilks'\n",
    "features_wilks = ['Sex', 'BodyweightKg', 'BestSquatKg', 'BestBenchKg', 'BestDeadliftKg', 'TotalKg']\n",
    "\n",
    "# Przygotowanie danych - usuwamy NaN w kolumnie celowej\n",
    "df_clean = df.dropna(subset=[target])\n",
    "df_clean = df_clean[features_wilks + [target]]\n",
    "\n",
    "# Podział\n",
    "train_data = df_clean.sample(frac=0.8, random_state=42)\n",
    "test_data = df_clean.drop(train_data.index)\n",
    "\n",
    "# Trening\n",
    "predictor = TabularPredictor(label=target).fit(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:50:51.695603Z",
     "start_time": "2026-01-18T12:44:16.110389Z"
    }
   },
   "id": "c82d70b02d03cd04",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root_mean_squared_error': np.float64(-1.3731866006821103), 'mean_squared_error': -1.8856414402928894, 'mean_absolute_error': -0.4092736794747087, 'r2': 0.9998113342295244, 'pearsonr': 0.9999057412094321, 'median_absolute_error': -0.23370666503905113}\n",
      "Przewidywany Wilks: 333.38\n"
     ]
    }
   ],
   "source": [
    "performance = predictor.evaluate(test_data)\n",
    "print(performance)\n",
    "\n",
    "# Test na nowych danych\n",
    "new_data = pd.DataFrame({\n",
    "    'Sex': [1],\n",
    "    'BodyweightKg': [72.0],\n",
    "    'BestSquatKg': [154.0],\n",
    "    'BestBenchKg': [116.0],\n",
    "    'BestDeadliftKg': [184.0],\n",
    "    'TotalKg': [454.0]\n",
    "})\n",
    "\n",
    "predicted_wilks = predictor.predict(new_data)\n",
    "print(f\"Przewidywany Wilks: {predicted_wilks[0]:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2026-01-18T12:56:48.508211Z",
     "start_time": "2026-01-18T12:56:20.973797Z"
    }
   },
   "id": "62c1c413972e9bfa",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
